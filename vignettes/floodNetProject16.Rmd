---
title: "FloodNet Project 1.6"
author: "Martin Durocher"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{FloodNet Project 1.6}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: inline
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, fig.align = 'center')

mycols <- c('#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c',
						'#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#ffff99','#b15928')

palette(mycols)

source(system.file("config", package = 'floodnetProject16'))
```

## Introduction

One objective of [FloodNet](http://www.nsercfloodnet.ca/) Project 1.6  is to provide Canadian engineers and hydrologist with a toolbox that can help them to perform flood frequency analysis.
To this end, a set of functions was developed and added to the R-package [CSHShydRology](https://github.com/floodnetProject16/CSHShydRology).
The package provides advanced users with a variety of functions to perform flood frequency analysis. 
Another tool available to the hydrologist community is the R-package [HYDAT](https://github.com/CentreForHydrology/HYDAT) that allows R to communicate with the [National Water Data Archive](https://www.canada.ca/en/environment-climate-change/services/water-overview/quantity/monitoring/survey/data-products-services/national-archive-hydat.html) maintained by the [Water Survey of Canada](https://www.canada.ca/en/environment-climate-change/services/water-overview/quantity/monitoring/survey.html).
This database can be downloaded and used to extract the necessary hydrological information.
The goal of the  R-package `FloodnetProject16` is to provide simple routines that extract directly the data from HYDAT and carry out flood frequency analysis according to FloodNet recommendations.

## Flood frequency analysis using annual maxima

The functionality of the R-package will be illustrated on the station '01AF009' that is located on the Iroquois River (NB).
In general, a flood frequency analysis consists of fitting a distribution of extreme events and evaluating the risk of occurrence for events of specific magnitudes.
Such risk is commonly quantified in terms of a return period $T$ defined as the expected waiting time between the extreme events. 
If flood risk does not change over time, _i.e._ we have a stationary time series, evaluating a return period is equivalent to find the quantiles of the distribution associated with the probability $1-1/T$. 
The example below extract the annual maxima of the station '01AF009' from HYDAT and output the results of the flood frequency analysis in the form of a table.


```{r}
library(floodnetProject16)
library(CSHShydRology)

## Station of interest and path of the HYDAT database
mystation <- '01AF009'
db <- DB_HYDAT
```

```{r}
set.seed(1)
FloodnetAmax(site = mystation, db = db, period = c(10,100))
```
More precisely, it fits the distribution using L-moments and evaluates its uncertainty using a parametric bootstrap technique. 
If not specified, the best distribution among the Generalized Extreme Value (GEV), Generalized Logistic (GLO), Generalized Normal (GNO) and Pearson type III (PE3) is selected on the basis of the Akaike Information Criterion (AIC).
The distribution with the lowest AIC is compared to the GEV. If the difference between the two criteria lower than two, they assumed to fit the data equally well and the GEV is preferred. Otherwise, the distribution with the lowest AIC is selected.

To use data from alternative sources, the argument `db` must be replaced by `x`.
In that case, the argument `site` is optional and may be used to specify a station name.
In the example below the function `AmaxData` is used to extract the Annual maxima of the station `01AF009` from HYDAT and pass to `FloodnetAmax`.


```{r}
set.seed(1)

an <- AmaxData('01AF009', db)
out <- FloodnetAmax(period = 100, x = an$value, out.model = TRUE, nsim = 0)
names(out)
```

The function `FloodnetAmax` is built on top of the function `FitAmax` of the R-package `CSHShydRology`.
If the argument `out.model = TRUE` the output of `FitAmax` is returned.
The example below shows that a return level plot can be easily obtained from the output model.

```{r fig.height = 4, fig.width = 6}
plot(out$fit, ci = TRUE)
```

By default the argument `verbose = TRUE`, which will perform further verifications to warn the user if there are fewer than 20 observations or if it detects trend or a change points according to the Mann-Kendall and Pettitt test (Helsel and Hirsch, 2002).

```{r}
## Create a change point to existing data
an.mod <- an$value + 50 * (1:nrow(an) > 14) 

set.seed(1)
out <- FloodnetAmax(x = an.mod, period = 100)
```

## Flood frequency analysis using peaks over threshold

An alternative to the analysis of annual maxima (AMAX) is Peak Over Threshold (POT).
For this approach, all exceedances above a given threshold are analyzed. 
As shown below, flood quantiles from POT can be obtained using the function `floodnetPot` that is built upon the function `FitPot` of the R-package `CSHShdRology`.

```{r}
set.seed(1)
FloodnetPot(site = mystation, db = db, period = 100, u = 20, area = 184.1)
```

The POT method requires a threshold `u` above which independent exceedances are extracted.
Here, the drainage area is needed to determine a minimal separating time between independent peaks as suggested by the US Water Resources Council (USWRC) (lang et al., 1999).
For the station `01AF009`, the drainage area is 184.1 $km^2$ and $u = 20$ was found to be a proper threshold leading roughly to an average of 2.07 peaks per year (PPY).
The Generalized Pareto (GPA) distribution is used to model the distribution of the exceedances.
The T-year flood quantiles of a POT model is defined as the flood quantiles of probability $1 - (\lambda T)^{-1}$ of the exceedance distribution, where $\lambda$ is the average number of peaks per year.

If the user wants to use data from another source, this must be passed as a data.frame where the first column is a date (_i.e._ class `Date`) and the second column is daily observations.
The function `DailyData` can be used to extract these hydrometric data in the proper format.


```{r}
set.seed(1)

daily <- DailyData(mystation, db)[,-1]

FloodnetPot(x = daily, period = 100, u = 20, area = 184.1, nsim = 0)

```

Generally, the POT analysis is based on more data than AMAX, which
is likely to reduce the uncertainty of the estimated flood quantiles. 
In this example, the standard deviation of the 100-year flood quantile of the POT model is 20.1 and was 26.8 for AMAX.

During the FloodNet project, 1114 stations of HYDAT were identified as having a natural flow regime and at least 20 years of observations.
The table `gaugedSites` of the present package contains some information collected during the project about these stations.
It includes among other some candidate thresholds.
One of these thresholds (column `auto`) is obtained automatically based on the p-value of the goodness-of-fit test of Anderson-Darting (Durocher et al. 2019). 
The other thresholds are associated with specific PPY.
For instance, the column `ppy175` is a threshold associated with approximately 1.75 PPY.
It must be pointed out that these thresholds were derived from data downloaded in August 2019.
The automatic threshold found for `01AF009` is 15.9, which is lower than the one previously used. 

```{r}
gaugedSites[5, c('station','description', 'area','auto','ppy250')]
```

If a threshold is not provided to the function `FloodnetPot`, it will be sought by the automatic method. 
On rare occasions where the drainage area ($A$) would also be unknown, it will be approximated by its empirical relationship with the mean daily flow ($M$).   

$$
\log(A) = 4.0934 + 0.9944 \, \log(M)
$$
The exemple below shows that the result of the POT analysis where the threshold was selected automatically.
The return level plot and the p-value of the goodness-of-fit test of Anderson-Darling suggest that GPA is a proper model for the exceedance.

```{r}
set.seed(1)
out <- FloodnetPot(period = 100, site = mystation, db = db, 
                                     area = 184.1, out.model = TRUE)

GofTest(out$fit, method = 'ad')

```

```{r, echo = FALSE, fig.height = 4, fig.width = 6}
plot(out$fit, ci = TRUE)
```

When `out.model = TRUE` and the threshold was selected automatically, the output will include additional information to validate the choice of the threshold.
The graphics below represent common diagnostics. 
For a well-chosen threshold, the Mean Residual Life plot should be approximately linear, which seems to be the case after `u=20`.

```{r, fig.width=6, fig.height=10}
uval <- out$u[out$u[,"ppy"] > 1.2,]
par(mfrow = c(3,1))

Fplot <- function(vname, mainLab, ylab){ 
  plot(uval[,'u'], uval[, vname], type = 'l', main = mainLab ,
       xlab = 'Threshold', ylab = ylab)
  abline(v = 15.9, col = 6, lwd = 2)
  abline(v = 20, col = 2, lwd = 2)
}

Fplot('mrl', 'Mean Residual Life', 'MRL')
legend('topright', col = c(6,2), lty = rep(1,2), 
             legend = c('auto','manual'))
Fplot('ad', 'Anderson-Darling', 'p-value')
Fplot('kap', 'Shape parameter', 'Kappa')
```
 

## Regional flood frequency analysis using AMAX

The quality of the flood quantile estimates for longer return periods depends heavily on the tail of the distribution. 
For common distributions employed in flood frequency analysis, this is controlled by a shape parameter. 
When there are only a few years of data for the site of interest, the variability of the estimates may be large.
To reduce it, Regional Frequency Analysis (RFA) was suggested to transfer information to the target site from a group of stations that has similar characteristics. 

The strategy recommended by FloodNet consists of using pooling groups corresponding to the nearest sites of the target, where the delineation is based on a similarity measure that accounts for the regularity and timing of the annual flood peaks (Mostofi Zadeh and Burn, 2019).
These stations can be taken from all available stations or from a larger group, called here super regions, that share common characteristics that are not part of the similarity measure.
The dataset `gaugedsites` contains pre-delineated super regions based on clustering techniques that have identified meaningful clusters of stations.
Both the hierarchical and k-means clustering methods are considered and use the same 4 variables: drainage area, mean annual precipitation (MAP), longitude and latitude.
In the following, the presentation of RFA method is done using the super regions of station '01AF009' proposed in column `supreg_km12` of the dataset `gaugedsites`, which used the k-means clustering method. 
The figure below displays the 12 resulting super regions in both the descriptor and the geographical space.


```{r, fig.height=10, fig.width = 6, echo = FALSE}
layout(matrix(c(1,2), 2,1))

sp::plot(map_ca)
axis(1)
axis(2)

points(lat~lon, gaugedSites, pch = 16, col = supreg_km12,
         main = 'Geographical space', ylim = c(42,72))

legend('top', horiz = TRUE, col = 1:12, legend = 1:12, pch = 16,
             cex = .6)

legend('bottomleft', pch = 10, legend = 'Target')

with(gaugedSites[5, ], 
         points(lon,lat, cex = 3, col = 'black', pch = 10))

plot(log(map)~ log(area), gaugedSites, pch = 16, col = supreg_km12,
         main = 'Descriptor space')

with(gaugedSites[5, ], 
         points(log(area), log(map), cex = 3, col = 'black', pch = 10))

```

Another information provided by `gaugedSites` is the p-values of common trend tests.
This can be used to remove stations presenting signs of nonstationary, _i.e._ stations where flood risk is changing over time.
For AMAX data, it includes the result of the nonparametric test of Mann-Kendall (`trend_mk`) and the Pettitt's test (`trend_pt`).
Overall, the information in `gaugedSites` is useful to select a super region for `01AF009` where only stationary stations are included.
In this case, the super region includes 107 stations, among which 100 are stationary.

```{r}
## Filter nonstationary sites from the super region of the target
target.supreg <- with(gaugedSites, supreg_km12[station == mystation])
cond_supreg <- with(gaugedSites, supreg_km12 == target.supreg)

pval.mk <- gaugedSites$trend_mk ## Mann-Kendall
pval.pt <- gaugedSites$trend_pt ## Pettitt
cond.trend <- pval.mk >= .05 & pval.pt >= .05
mysites <- gaugedSites[cond_supreg & cond.trend,'station']

addmargins(table(cond_supreg, cond.trend))
```


First, hydrometric data for the pooling group must be prepared in the form of a dataset.
As shown earlier, this information can be extracted by passing to the function `AmaxData` all stations in the super regions.
Optionally, a target can be supplied to return only the stations that are part of the pooling group of a given target. 
The user can also provide a custom measure of similarity in the form of a vector or a distance matrix.

```{r}
season.dist <- SeasonDistanceData(mysites, db)

xd <- AmaxData(mysites, db, target = mystation, size = 25,
                             distance = season.dist)
```

The RFA analysis of a target site can be performed using the function `FloodnetPool`that is built upon the function `FitRegLmom` of the R-package `CSHShydRology`.
The output of the RFA is presented below and the method is briefly explained in the following.
It can be noted that RFA appears to have improved the estimation as the standard deviation of the 100-year flood quantile.

```{r}
set.seed(1)
out <- FloodnetPool(x = xd, target = mystation, 
                                 period = 100, distr = 'gev', out.model = TRUE, verbose = FALSE)
print(out$fit)

```

```{r}
print(out$qua)
```

In brief, the function fits an index-flood model (IFM) using the L-moment algorithm.
IFM assumes that inside a homogenous region, all distributions are proportional up to a scaling factor. 
Here this factor is taken as the sample average.
One consequence of this hypothesis is that the coefficient of variation must be the same for all sites.
The heterogeneous measure $H$ (Hosking and Wallis, 1997) of a pooling group represents the variability of the L-coefficient of variation (LCV), which can be used for judging the veracity of the IFM hypothesis. 
Also, if not specified, the best distribution among GEV, GLO, GNO and PE3 is selected using the Z-statistic (Hosking and Wallis, 1997) that identify the distribution where the theoretical L-kurtosis best match the theoretical one.

The function `FloodnetPool` starts with an initial pooling group of size 25.
If $H > 2$ the pooling group is considered heterogeneous and should be updated.
In turn, each neighboring site is removed and $H$ is re-evaluated. 
The station leading to the largest improvement in $H$ is removed and the process is repeated until $H \leq 2$. 
A  stopping criterion is used to ensure that at least 5T station-years are found in the pooling group where T is the return period (Robinson and Reed, 1999).
Please note that the function `FloodnetPool` uses the largest return period requested to evaluate the stopping criterion.
For example, to evaluate a 100-year return period, at least 500 station-years are necessary.
A warning is issued if it fails to encounter a pooling group where $H \geq 2$.

The standard deviation and the confidence intervals of the flood quantile are estimated using parametric bootstraps.
The simulations are obtained from a multivariate normal distribution and the marginal distributions are adjusted according to the at-site distribution. 
The correlation coefficients of the normal distribution represent the average value of all pairs of sites.

An alternative to the L-moments algorithm for fitting an RFA model is to optimize the independent likelihood of the model, _i.e._ the likelihood of the RFA model assuming that all observations are independent. 
This fitting can be done using the function `FloodnetPoolMle` that works like `FloodnetPool`, but that is using the underlying function `FitPoolMle` of the R-package `CSHShydRology`.  
One difference in the call of this function is the argument `type` that specified the type of the regional model. 
The IFM model can be specified using `type = 'mean'`, which will imply that all sites are standardized by the empirical mean and the parameters of the regional distribution are estimated using the likelihood technique.  
A similar model is specified by `type = 'cv'`, which assumed that all sites share the same distribution with a common shape parameter. For each site, the model estimates a different location parameter and
the scale parameter is proportional to the location parameter up to a regional parameter that plays a similar role to a coefficient of variation.
A third model resulting from choosing `type = 'shape'`, assumes that all sites have a common shape parameter with different location and scale parameters for each site. 

Other differences with `FloodnetPoolMle` are that the pooling group is not updated according to its homogeneity and the best distribution is selected as the one having the highest independent likelihood. 
The example below fit the RFA-AMAX model using the likelihood approach, which takes some time due to the demanding bootstrap procedure. 


```{r}
set.seed(1)
FloodnetPoolMle(x = xd, target = mystation, period = 100,
                                type = 'mean', nsim = 500)

FloodnetPoolMle(x = xd, target = mystation, period = 100,
                                type = 'cv', nsim = 0)

FloodnetPoolMle(x = xd, target = mystation, period = 100, 
                                type = 'shape', nsim = 0)

```


## Regional frequency analysis using POT

The function `FloodnetPool` can also be used to carry out RFA using POT.
Similarly, the information in `gaugedSites` about trend tests and super regions can be used to select an initial set of stations. 
In this case, the Mann-Kendall test for the exceedances (column `trend_mx`) and logistic regression model (column `trend_lg`) that looks respectively at potential trends in the mean excess and the exceedance probability can be used to remove nonstationary stations.
Below we observed that 96 stations of the super regions respect these criteria.

```{r}
## Filter nonstationary sites from the super region of the target
pval.mx <- gaugedSites$trend_mx ## Mann-Kendall
pval.lg <- gaugedSites$trend_lg ## logistic regression
cond.trend <- pval.lg >= .05 & pval.mx >= .05

info <- gaugedSites[cond_supreg & cond.trend, c('station','auto','area')]

season.dist <- SeasonDistanceData(info$station, db)

head(info,3)
```

```{r}
addmargins(table(cond_supreg, cond.trend))
```

For performing RFA using POT the input data must be an object of the class `peaksdata`.
The easiest way to do so is to use the function `DailyPeaksData` that directly the data from HYDAT in the right format. 
A target can also be specified to limit the output to a specific pooling group.
If the peaks were obtained from another source, the right input can be obtained by passing the peaks to the function `PeaksData` along with the required information, _i.e._ the threshold used and the total number of years.  

```{r}
xd <- DailyPeaksData(info, db, target = '01AF009', 
                                         size = 25, distance = season.dist)

```

The code below fits the POT-IFM model and return the flood quantiles.
In this case, regionalization has reduced the variability of the 100-year flood quantiles in comparison with the at-site POT model.

```{r}
set.seed(1)
out <- FloodnetPool(xd, target = mystation, period = 100, verbose = TRUE,
                         out.model = TRUE)

out$qua
```

In the previous example, the intersite correlation was not considered in the estimation of the flood quantiles, which may lead to underestimating the variability of the estimated flood quantiles.
It should be noted that if peaks are passed in `FloodnetPool` as a data.frame, they will be treated as AMAX data.
Therefore, one could obtain flood quantile estimates by selecting the GPA and impose a desired intersite correlation structure.
Note that the return period will need to be multiplied by the target PPY to obtain the proper exceeding probability and a 3 parameter GPA is used.
The example below shows the standard deviation obtained using that approach with a correlation coefficient of 0.5.

```{r}
ppy <- with(xd, npeak[1]/nyear[1])
xp <- xd$peaks

set.seed(1)
out <- FloodnetPool(xp, target = mystation, distr = 'gpa', tol.H = Inf,
                                        period = 100 * ppy, verbose = FALSE, corr = .5)
```

Another way of fitting the POT model is to use the likelihood approach.
The results below were obtained by passing the argument `type = 'shape'` which specifies GPA distributions where site-specific scale parameters are used with a common shape parameter.


```{r}
set.seed(1)
FloodnetPoolMle(xd, target = mystation, period = 100, nsim = 500, type = 'shape')
```



## Prediction at ungauged basins

When there is no hydrometric data at the site of interest, frequency analysis cannot be done by fitting directly a distribution of extreme events. 
The quantile regression techniques (QRT) is a method that is used in this situation to predict flood quantiles based on the characteristics of the site of interest.
First, it evaluates the flood quantiles of gauged stations using at-site information.
Next, a regression model is used to predict the flood quantile at a site of interest (ungauged) based on its descriptors.

Similar to the dataset `gaugedSites`, this package includes a dataset `descriptors` that contains meteorological and physical characteristics for 770 stations that are also in `gaugedSites`. 
This information can be used to fit a QRT model that predicts flood quantiles at sites of interest.
In the example below, we create a dataset including 6 descriptors: Drainage area (AREA), mean annual precipitation (MAP), percentage of waterbodies (WB), stream density (STREAM), elevation (ELEV) and slope (SLOPE).


```{r}
xd <- with(descriptors,
  data.frame(
  	site = station,
    area = log(area),
    map  = log(map_ws),
    wb   = log(.01 + wb),
    stream = log(.01 + stream),
  	elev = elev_ws,
  	slope = log(.01 + slope)))

coord <- descriptors[, c('lon', 'lat')]

```
Logarithmic transformations are used above for most variables to make the input variables more normally shaped.
In addition, multidimensional scaling is used here to project the geographical coordinates in a Cartesian space that aims to preserve the great-circle distance.

In this section, the station `01AF009` is treated as ungauged and the objective is to evaluate the 100-year flood quantile (Q100) according to its descriptors.
Remember that we derived a Q100 of 98.0 $m^3/s$ from an at-site flood frequency analysis of the annual maxima.  

```{r}
target.id <- (xd$site == '01AF009')

target <- xd[target.id,]
xd <- xd[-target.id,]
```



The function `FloodnetRoi` is built upon the function `FitRoi` in `CSHShydRology` and performs flood frequency at ungauged sites using QRT. 
First, it extracts the annual maxima of HYDAT and evaluates the flood quantiles $q_i$ of each gauged station ($i$). 
Next, a locally log-linear model is applied to the nearest stations to the target $j$:
$$
\log(q_j) = \mathbf{X}_j\beta_j + e_j
$$
where $\mathbf{X}_j$ is a design matrix of descriptors, $\beta_j$ is local parameters and $e_j$ is a term of error.
The distance between sites is taken as the Euclidean distance between standardized descriptors.
Also, weights proportional to the Epanechnikov kernel are used to give more importance to more similar sites.
Please note that the support of the Epanechnikov kernel is bounded and hence the bandwidth parameter that controls the relative importance of the nearest sites is expressed in terms of the number of gauged stations inside these bounds, which corresponds to the size of the region of influence (`size`). 
If more than one size is provided, `FloodnetRoi` uses 10-fold cross-validation to find the parameter that optimizes the Mean Absolute Deviation (MAD).
The results of the cross-validation scheme can be obtained by passing the argument `out.model = TRUE`.
The figure below shows that sizes between 60 and 90 stations are optimal.

```{r}
set.seed(1)
out <- FloodnetRoi(target = target, sites = xd,
                        db = db, period = 100, size = seq(25, 200, 10), 
                        nsim = 0, verbose = FALSE, out.model = TRUE)
```

```{r, echo = FALSE, fig.height = 4, fig.width = 6}
plot(out$cv, 'mad', ylab = 'Mean Absolute Deviation (MAD)', 
		 main = 'Cross-validation')
```

The function `FloodnetRoi` uses a bootstrap technique to evaluate the uncertainty of the estimated flood quantiles.
As done previously for the RFA using gauged sites, annual maxima are simulated using a multivariate normal distribution with marginal transformed to the respective at-site distributions. 
This leads to a bootstrap sample of flood quantiles $q_i^\ast$ at each gauged site $i$. 
In parallel, prediction errors $e^\ast_i$ are sampled (balance bootstrap) from the residuals of the QRT model based on 10-fold cross-validation.
The final bootstrap sample is composed of the flood quantiles values 
$\q_i^{\ast\ast} = \q_i^\ast+e^\ast_i$ that account for the modeling error of the QRT model and the sampling error due to the at-site estimation of the flood quantiles.

```{r}
set.seed(1)
FloodnetRoi(target = target, sites = xd, db = db, 
						period = 100, size = 85, nsim = 100)
```

Moreover, the set of existing site characteristics does not fully characterize the relationship between the descriptors and the flood quantiles. 
In particular, several potentially missing descriptors could be spatially distributed, which would lead to spatially correlated residuals.
If coordinates are provided to the function `FloodnetRoi`, it will further perform a simple kriging technique on the residual of the QRT to improve the prediction of the flood quantiles when there is spatial correlation among them. 
The example below shows that the criterion MAD improves from 38.0 to 34.0 when the kriging step is added to the QRT model. 


```{r}
## Extract the coordinates
coord <- descriptors[, c('lon', 'lat')]
coord <- as.data.frame(cmdscale(GeoDist(coord), 2))

target.coord <- coord[target.id,]
coord <- coord[-target.id,]
```


```{r}
set.seed(1)
out <- FloodnetRoi(target = target, sites = xd,
                        target.coord = target.coord, sites.coord = coord,
                        size = 209:211, db = db, period = 100, 
                        nsim = 0, verbose = FALSE, out.model = TRUE)

head(out$cv)
```

## Conclusion

In summary, this document showed how the R-package `floodnetProject16` can be used to perform flood frequency analysis using the hydrometric data found in the HYDAT database.
The function `FloodnetAmax` and `FloodnetPot` were shown to carry out at-site frequency analysis based on annual maximum and peaks over threshold.
The functions `FloodnetPool` and `FloodnetPoolMle` were used to obtain flood quantiles from regional analysis based on either AMAX and POT.
Finally, it was shown how evaluation of flood quantiles at ungauged sites can be performed using the function `FloodnetRoi`.

Two datasets : `gaugedSites`, `descriptors` were presented
that contain useful information derived from HYDAT. In particular, they suggest candidate thresholds for POT analysis, super regions for delineating pooling groups, trend tests to identify stationary stations and site descriptors to predict hydrological variables at ungauged sites. 
Some of this information will become outdated, but represent a good starting point.
Please see the extra materials that contain more codes examples.


## References

* Durocher, M., Burn, D. H., & Mostofi Zadeh, S. (2018). A nationwide regional flood frequency analysis at ungauged sites using ROI/GLS with copulas and super regions. Journal of Hydrology, 567, 191–202. https://doi.org/10.1016/j.jhydrol.2018.10.011

* Durocher, M., Zadeh, S. M., Burn, D. H., & Ashkar, F. (2018). Comparison of automatic procedures for selecting flood peaks over threshold based on goodness-of-fit tests. Hydrological Processes. https://doi.org/10.1002/hyp.13223

* Durocher, M., Burn, D. H., Zadeh, S. M., & Ashkar, F. (2019). Estimating flood quantiles at ungauged sites using nonparametric regression methods with spatial components. Hydrological Sciences Journal, 64(9), 1056–1070. https://doi.org/10.1080/02626667.2019.1620952

* Helsel, D. R., & Hirsch, R. M. (2002). Statistical Methods in Water Resources. In Techniques of Water-Resources Investigations of the United States Geological Survey. Retrieved from http://water.usgs.gov/pubs/twri/twri4a3/

* Lang, M., Ouarda, T. B. M. J., & Bobée, B. (1999). Towards operational guidelines for over-threshold modeling. Journal of Hydrology, 225(3), 103–117. https://doi.org/10.1016/S0022-1694(99)00167-5

* Mostofi Zadeh, S., & Burn, D. H. (2019). A Super Region Approach to Improve Pooled Flood Frequency Analysis. Canadian Water Resources Journal / Revue Canadienne Des Ressources Hydriques, 0(0), 1–14. https://doi.org/10.1080/07011784.2018.1548946

* Robson, A., & Reed, D. (1999). Flood estimation handbook. Institute of Hydrology, Wallingford.


