---
title: "A guide to flood frequency analysis using floodnetRfa"
author: "Martin Durocher"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{FloodNetRfa}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, fig.align = 'center', fig.height = 4, fig.width = 6)

source(system.file("config", package = 'floodnetRfa'))
```

## Introduction

One objective of [FloodNet](http://www.nsercfloodnet.ca/) is to provide Canadian engineers and hydrologists with a set of tools that allows them to perform flood frequency analysis (FFA) easily and accurately.
To this end, common methods in FFA were investigated and implemented in the R-package [CSHShydRology](https://github.com/floodnetProject16/CSHShydRology).
Another tool available to the Canadian water science community is the R-package [HYDAT](https://github.com/CentreForHydrology/HYDAT) that simplifies the communication between R and a local version of the [National Water Data Archive](https://www.canada.ca/en/environment-climate-change/services/water-overview/quantity/monitoring/survey/data-products-services/national-archive-hydat.html) maintained by the [Water Survey of Canada](https://www.canada.ca/en/environment-climate-change/services/water-overview/quantity/monitoring/survey.html).
The R-package `floodnetRfa` is built on the top of these two R-packages and 
aims to create en coherent environment for applying Floodnet recommendations.
The package includes instructions that can be invoked directly in the R terminal or via a graphical interface.

In this document, we assume that the reader is familiar with the basic concepts of FFA  and we show the general capabilities of `floodnetRfa` within the terminal interface.
To illustrate common situations the station 01AF009, located on the Iroquois River in New-Brunswick, serves as a case study.
The code below is used to initiate the working environment.

```{r echo = FALSE}
## Code run but not displayed
library(floodnetRfa)

## Studied station
STN <- '01AF009'
```


```{r eval = FALSE}
library(floodnetRfa)

## Studied station
STN <- '01AF009'

## Global variable that represent the path of the HYDAT database
## Must be set by the user.
DB_HYDAT <- ".../Hydat.sqlite3"

```


## Reading data

There are three main types of function that works together in `floodnetRfa` to performs FFA.
The core functions are the modeling functions that fit the desired statistical models.
Those functions are normally preceded by data extracting functions that prepare the hydrometric data.
Finally, the last type of functions are interpreting functions that assess the component of the fitted models.

All modeling functions require that the hydrometric data be organized in a dataset (`data.frame`) with three columns: `site`, `date` and `value`.
In particular, the column `date` must be in a standard `Date` format.
In most situations, an extracting function exist to import directly hydrometric data from HYDAT in the correct format.
For instance, the function `AmaxData` and `DailyDate` extracts annual maximum discharges and daily streamflows.
Nevertheless, if the data comes from a different source, it is recommended to use the function `SequenceData` to convert the data into the desired format. 
The examples below illustrate common situations where the function `SequenceData` is used to prepare datasets from existing numerical vectors.

```{r}
## Example of a yearly sequence, starting today.
mydata <- rnorm(3)
SequenceData(mydata, site = 'mysite')


## Example of a daily sequence starting in 2000-01-01.
SequenceData(mydata, freq = 'days', sdate = '2000-01-01')


## Example of an irregular sequence.
mydate <- as.Date(c('2000-01-01','2005-05-05','2010-10-10'))
SequenceData(mydata, mydate)

## Create a testing set
SequenceData(3, site = c('s1','s2'))
```

Note that when the input data is a single integer `n` the function `SequenceData` simulate a Gumbel distribution of size `n` with mean 100 and standard deviation 30. 
This can be useful to quickly create testing datasets.

A particular type of hydrometric data are exceedances, _i.e._ values above a given threshold.
In this case, the prepared dataset must have the same format, but the meta-information related to the thresholds must be added. 
In the example below, the function `PeaksMeta` links the thresholds and exceedance rates, expressed in an average number of peaks per year (PPY), to the dataset of exceedances.
As shown, the required format for the meta-information is a dataset with exactly 3 columns: site, thresh and ppy.
One can see that the function `PeaksMeta` is also able to extract the meta-information from a dataset of exceedances.
```{r}
## Create hydrometric data from two sites
set.seed(1)
mysite <- c('s1','s2')
xd <- SequenceData(3, site = mysite)

## Define and add meta-information
meta <- data.frame(site = mysite, thresh = c(175,160), ppy = c(1.5,2.5) )

PeaksMeta(xd) <- meta

## See the results
head(xd)
PeaksMeta(xd)
```

## Flood frequency analysis using annual maxima

The two main steps to carry out FFA are to fit a statistical distribution of extreme events and to evaluate flood levels associated with given probabilities.
For a classical AMAX analysis, the extreme events are annual maxima, while in the peaks over threshold (POT) approach the extreme events are exceedances. 
Flood risk is commonly quantified in terms of a return period $T$ defined as the expected waiting time between two extreme events. 
When assuming that the exceeding probability is constant over time, the evaluation of a return period is equivalent to estimate the flood quantiles of probability $1-1/T$ for AMAX and $1-(\lambda T)^{-1}$ for POT where $\lambda$ is the exceedance rate.
The example below extracts annual maxima from HYDAT using the function `AmaxData` and performs FFA using the function `FloodnetAmax`.
The argument `period` specifies the return period that are estimated.

```{r, eval = FALSE}
an <- AmaxData(DB_HYDAT, STN)
fit <- FloodnetAmax(an, period = c(10,100))
```

The function `FloodnetAmax` fits a list of candidate distributions using L-moments, choose one according to the Akaike Information Criterion (AIC) and evaluate the flood quantiles. 
In the end, a parametric bootstrap method is used to measure the uncertainty of the fitted model. 
If no list of distributions is passed, the distribution with the lowest AIC is identify among the Generalized Extreme Value (GEV), Generalized Logistic (GLO), Generalized Normal (GNO) and Pearson type III (PE3).
The identified distribution is then compared to the GEV and prefered only if the difference of AIC greater than 2.
The idea behind that procedure is that for a difference less than 2, the two distributions can be seen as having similar fits.
Therefore GEV is prefered as it represent the asymptotic distribution of sample maxima (Coles, 2001).

The pipe operator `%>%` can be employed to obtain a syntax familiar to the [tidyverse](https://style.tidyverse.org/pipes.html).
The rest of the document adopts this syntax, because it is easier to read and avoid the creation of an intermediate variable.
The example below produce the same results as the earlier code.
 
```{r}
set.seed(1)
fit <-  DB_HYDAT %>%
	AmaxData(STN) %>%
	FloodnetAmax(period = c(10, 100))
```

The output of a modeling function is a S3 object of the class `floodnetMdl` for which 
interpreting functions are available to access the various component of the fitted model.
The function `print` display only a brief description of the fitted model, while `summary` add information about the flood quantiles and model parameters.
For the 100-year flood quantile (Q100), the AMAX method estimates a streamflow of 98 $m^3/s$ with a standard deviation of 27. 

```{r}
summary(fit)
```

Another way to access this information is to convert the output into a dataset using the function `as.data.frame`.
The argument `type = 'p'` is used to return the model parameters instead of the flood quantiles (`type = 'q'`).
In this form, the results from multiple fitted models can be merged easily.

```{r}
## Data flood quantiles
head(as.data.frame(fit), 4)

## 
sim <- SequenceData(50) %>%
	FloodnetAmax(distr = 'pe3', nsim = 0, verbose = FALSE) %>%
	as.data.frame('p')

## Merged results 
rbind(as.data.frame(fit, 'p'), sim)
```

The function `FloodnetAmax` is built on top of the function `FitAmax` of the R-package `CSHShydRology`.
If the argument `out.model = TRUE` is used, the output of `FitAmax` is also returned.

```{r}
fit0 <- DB_HYDAT %>%
	AmaxData(STN) %>%
	FloodnetAmax(out.model = TRUE)

fit0$fit	

```

## Trend tests

The methods developed in the `floodnetRfa` package are not adapted to the estimation of time-varying flood quantiles.
Therefore, it is important to verify that trends are not present in the data.
The function `floodnetAmax` automatically performs the Mann-Kendall's and Pettitt's test to examine the likelihood of trends or change points in the data (Helsel and Hirsch, 2002) and issues warnings when it fails. 
Such warnings indicate that the model assumptions must be further verified. 

```{r}
## Create a change point to existing data
set.seed(2)
an <- SequenceData(100)
mid <- 1:nrow(an) > 50
an$value[mid] <- an$value[mid] + 50  

## Perform FFA
out <- FloodnetAmax(an)
```

However, if the user is confident in the validity of the chosen procedure, these warnings can be silenced by setting the argument `verbose = FALSE`.

## Diagnostic plots

Various graphics can be obtained from the `floodnetMdl` object using the function `plot`.
The generated plots are created using the grammar of graphics `ggplot2`.
By default, the return level plot is returned to compare the fitted flood quantiles (red line) with the observations. 

```{r}
plot(fit)
```

The elements of the plot, like lines (`geom_line`) and points (`geom_point`), can be modified by passing a list of arguments to the underlying geometry.
Moreover, the graphic can also be further customized using the ggplot syntax.
The following example modified several elements of the same previous graphics.

```{r}
library(ggplot2)

plot(fit, line.args = list(colour = 'red', size = 1), 
		 point.args = list(colour = 'blue', size = 2),
		 ribbon.args = list(fill = 'pink'),
		 ylab = 'Flood quantiles') +
	theme_classic() + labs(title = 'Customized version')
	
```

Another plot useful for assessing the fitting of a distribution is the QQ-plot that displays the theoretical versus the sample quantiles of a distribution.
The QQ-plot differs from the return level plot by using a different x-axis that results in a linear relationship when a proper model is used.

```{r}
plot(fit, 'qq')
```

A histogram of the observed data and the fitted density can be obtained using the function `hist` or `plot` with argument `type = 'h'`. 
With this graphic, the p-value of the goodness-of-fit test of Anderson-Darling is reported as an additional assessment of the validity of the selected distribution.
For station 01AF009, the hypothesis of a GEV distribution cannot be rejected.

```{r}
hist(fit, histogram.args = list( bins = 15))
```

As mentioned, warnings are issued when the modeling function detects trends and change points.
To help with this examination, the function `plot` with the argument `type = 't'` displays the Sen's slope and includes the p-value the tests of Mann-Kendall and Pettitt.
For station 01AF009, the two tests do not suggest a significant trend in the data.

```{r}
plot(fit, 't')
```


## Flood frequency analysis using peaks over threshold

As an alternative to AMAX, POT can be carried out using the function `DailyData` to extract
daily streamflow and the function `FloodnetPot` to perform the FFA.
The latter modeling function is built on top of the function `FitPot` of the R-package `CSHShdRology`.

```{r}
set.seed(1)

fit <- DB_HYDAT %>%
    DailyData(STN) %>%
    FloodnetPot(period = 100, u = 20, area = 184)
```

In comparison to the AMAX method, the distribution of the POT method is known and corresponds to a Generalized Pareto (GPA) distribution.
The model has 2 parameters but requires also a threshold `u` above which independent peaks are extracted.
The minimal separating time between independent peaks is determined by the drainage area of the basin as suggested by the US Water Resources Council (USWRC) (lang et al., 1999).
For station 01AF009, the drainage area is 184 $km^2$ and a threshold $u = 20$ was selected.
In the end, a parametric bootstrap strategy evaluates the uncertainty of the flood estimates.

Identically to `floonetAmax`, the output of `floodnetPot` is a `floodnetMdl` object.
Therefore, the same interpreting functions are used to extract modeling information and create graphics.

```{r}
summary(fit)
plot(fit)
```

In complement to POT analysis, the Mann-Kendall's test is considered to examine trends in the exceedances and potential changes in the exceedance rate are examined using logistic regression models with time as a covariate.
More precisely, F-tests are applied logistic model with polynomial trend of order 1 to 3 and the lowest p-value is returned.

```{r}
plot(fit, 't')
```

## Automatic selection of the threshold

If a threshold is not provided to the function `FloodnetPot`, one will be selected automatically based on the p-value of the goodness-of-fit test of Anderson-Darting (Durocher et al. 2018b).
During the investigation conducted by the FloodNet team, 1114 stations were identified as having a natural flow regime and at least 20 years of observations.
The table `gaugedSites` contains information collected about these stations, which includes among other candidate thresholds.
The column `auto` represent the thresholds obtained from the automatic selection method.
For the station 01AF009, the automatic threshold selected a threshold of 15.9, which is lower than the one previously used. 
The other thresholds are available and are associated with specific PPY.
For instance, the column `ppy175` is a threshold associated with 1.75 PPY.

```{r}
gaugedSites[5, c('station','description', 'area','auto','ppy250')]
```

## Comparison plots

It is often interesting to try more than one model before making a final decision.
The examples below show how the function 'CompareModel' allows to easily creates plots that compare the confidence intervals
and coefficient of variation of the AMAX and POT results.
In this case, we are seeing that for return periods longer than 10 years POT is more accurate than AMAX and the opposite is true for equal or shorter return periods.

```{r}
set.seed(1)
fit.amax <-  DB_HYDAT %>%
	AmaxData(STN) %>%
	FloodnetAmax()

fit.pot <- DB_HYDAT %>%
    DailyData(STN) %>%
    FloodnetPot(u = 20, area = 184)

lst.fit <- CompareModels(fit.amax, fit.pot)

plot(lst.fit)
plot(lst.fit, 'cv')

```


## Super regions

The quality of flood quantile estimates for longer return periods depends heavily on the tail of the selected distribution.
Most of the distributions used in FFA possess a shape parameter that allows controlling that aspect of the distribution. 
However, when only a few years of data are available at the site of interest, the uncertainty associated with this parameter may be large.
To reduce it, Regional Frequency Analysis (RFA) is recommended to transfer additional information from nearby stations with similar properties. 

The strategy recommended by FloodNet is to use pooling groups delineated by a similarity measure that accounts for the regularity and timing of the annual flood peaks (Mostofi Zadeh and Burn, 2019).
The sites forming the pooling groups are selected from an initial set of stations, or super region, that are relevant to the scope of the analysis.
Although it is common to consider administrative boundaries(_e.g._ provinces), more hydrologically relevant super regions can be built by regrouping stations with similar hydrological properties. 

The dataset `gaugedSites` contains pre-delineated super regions based on 4 widely available catchment descriptors:
    
    * Drainage area
    * Mean annual precipitation (MAP)
    * Longitude
    * Latitude 

In the rest of the document, the super region proposed in column `supreg_km12` of the dataset is considered.  
The division of the stations into 12 super regions was obtained using the k-means algorithm.
The figures below present the resulting super regions in the geographical, seasonal and descriptor spaces.
The function `MapCA` and `SeasonPlot` are provided in `floodnetRfa` to simplify the creation of a simple map in the respective spaces.


```{r}
## Extract data
xd <- gaugedSites[, c('station', 'lon', 'lat')]

xd$area <- log(gaugedSites$area)
xd$map <- log(gaugedSites$map)
xd$region <- as.factor(gaugedSites$supreg_km12)
xd$theta <- gaugedSites$season_angle
xd$r <- gaugedSites$season_radius

## Function that customize graphics
FormatPlot <- function(plt, main) {
	
	## Define a custom palette
	mycolors = c('#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c',
						 '#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#ffff99','#b15928')

	plt + scale_colour_manual(values = mycolors) + 
	  theme(legend.pos = '', plot.title = element_text(hjust = 0.5)) + 
		ggtitle(main)
}

## Maps
plt <- MapCA(polygon.args = list(fill = 'white', colour = 'grey')) + 
	geom_point(data = xd, aes(x = lon, y = lat, colour = region)) +
	geom_point(data = xd[5,], aes(x = lon, y = lat), 
						 colour = 'black', size = 5, shape = 10)

FormatPlot(plt, 'Geograpical space')
  	

## Seasonal plot
plt <- SeasonPlot() +
	geom_point(data = xd, aes(x = theta, y = r, colour = region))

FormatPlot(plt, 'Seasonal space')


## Descriptor space
plt <- ggplot() + 
	geom_point(data = xd, aes(x = area, y = map, colour = region)) +
	xlab('Drainage area (log)') + ylab('Mean annual precipitation (log)')
	
FormatPlot(plt, 'Descriptor space') 

```

Another information contained in the dataset `gaugedSites` is the p-values of common trend tests.
It includes the result of Mann-Kendall's (`trend_mk`) and Pettitt's (`trend_pt`) tests.
In the context of POT, the test of Mann-Kendall applied on the exceedances (`trend_mx`) and logistic regression are considered (`trend_lg`).

Overall, the dataset `gaugedSites` contains sufficient information to select a super region that is hydrologically relevant to the target site and for which the assumption of constant flood risk over time appears reasonable. 
The function `GetSuperRegion` simplifies the task of extracting the super regions suggested by `gaugedSites` for a given site. 
Note that due to the different trend tests, the result may differ slightly depending on the type of analysis (AMAX/POT).
For POT, the drainage area and a selected threshold returned as they are necessary to evaluate the exceedances.
 
```{r}
## AMAX super region
sreg.amax <- GetSuperRegion(STN)

## POT super regions with automatically selected threshold
sreg.pot <- GetSuperRegion(STN, type = 'pot')

## POT super regions with 2-PPY thresholds
sreg.pot2 <- GetSuperRegion(STN, type = 'pot', trend.tol = 0.1, thresh = 'ppy200')
```

## Regional flood frequency analysis

RFA is performed using the function `FloodnetPool` and it needs as input hydrometric data in the same format as the previous modeling function.
When the argument `target` is passed to `AmaxData`, the function extracts only the data of the desired pooling group.
By default, the recommend similarity measure based on the seasonality of the annual maxima serves in the delineation of the pooling group. 
Alternatively, a user-defined distance can be passed.
In this case, the target site is identified as the unique site having a distance of zero.


```{r}
set.seed(1)

## Using seasonal distance
fit <- DB_HYDAT %>% 
    AmaxData(sreg.amax, target = STN) %>%
    FloodnetPool(target = STN, verbose = FALSE)
    
```

```{r, eval=FALSE}
## Using Euclidean distance
sid <- gaugedSites$station %in% sreg.amax
euclid <- gaugedSites[sid, c('area','map')] 
euclid <- dist(scale(log(euclid)))
euclid <- as.matrix(euclid)[5,]

DB_HYDAT %>% 
    AmaxData(sreg.amax, distance = euclid) %>%
    FloodnetPool(target = STN, verbose = FALSE)

```

The modeling function is built on top of the function `FitRegLmom` of the R-package `CSHShydRology`, which fits an index-flood model (IFM) using the L-moment algorithm (Hosking and Wallis, 1997).
IFM assumes that inside a homogenous region, all distributions are proportional up to a scaling factor that corresponds to the sample average.
One consequence of this assumption is that the coefficient of variation of all sites must be the same.
The heterogeneous measure $H$ of a pooling group represents the variability of the L-coefficient of variation (LCV) and serves as a criterion for judging the likelihood of the IFM hypothesis. 
If $H > 2$ the pooling group is most likely heterogeneous and should be updated.
To this end, each neighboring site is removed in turn and $H$ is re-evaluated. 
The station leading to the largest improvement is permanently removed and the process is repeated until $H \leq 2$. 
A second stopping criterion imposes that at least 5T station-years are found in the pooling group, where T is the desired return period (Robinson and Reed, 1999).
If multiple return periods are provided, the function `FloodnetPool` uses the largest return period requested to evaluate that stopping criterion.
For example, to evaluate a 100-year return period, at least 500 station-years are necessary. 
In the end, a warning is issued if the function fails to encounter a pooling group respecting both stopping criteria.

The standard deviation and the confidence intervals of the flood quantile are estimated using parametric bootstraps.
The simulations are generated by a multivariate normal distribution with marginal distributions representing the at-site estimates. 
The correlation matrix has a constant coefficient taken as the pairwise average of all sites.
If not specified, the best distribution is selected using the Z-statistic (Hosking and Wallis, 1997) among GEV, GLO, GNO and PE3.
This criterion identifies the distribution having the theoretical L-kurtosis that best matches the theoretical one, knowing the sample L-skewness.

In addition to the previous interpreting function `print`, `summary` and `plot` that assess the fitting of the target sites, the L-moment ratio diagram is obtained using the argument `type = 'l'` with the function `plot`. 

```{r}
summary(fit)

plot(fit, 'l')
```

If the input hydrometric data are exceedances, the RFA will be performed using a GPA distribution and account for the threshold when it evaluates the flood quantiles.
The function `ExtractPeaksData` can be used to extract independent peaks from multiples stations.
However, if the hydrometric data are coming from HYDAT, it is easier to use the function `DailyPeaksData` that performs as a single instruction the task of importing the hydrometric (`DailyData`) and extracting the independent peaks (`ExtractPeaksData`).
Like the function `AmaxData`, a target can be provided to limit the imported data to the desired pooling group.

```{r}
fit <- DB_HYDAT %>%
	DailyPeaksData(info = sreg.pot, target = STN) %>%
	FloodnetPool(target = STN, verbose = FALSE)
	
summary(fit)
```

## Prediction at ungauged basins

When there is no hydrometric data at the site of interest, FFA cannot be done simply by fitting a distribution. 
Instead, the quantile regression techniques (QRT) is a method that can be adopted to estimate flood quantiles based on available catchment descriptors.
The package the dataset `descriptors`, included in this package, contains meteorological and physical characteristics of 770 stations found in `gaugedSites`.
This information can be used to fit a QRT model when the same descriptors are available at the ungauged site.

In this section, the station 01AF009 is treated as ungauged and the objective is to evaluate the 100-year flood quantile (Q100) according to its descriptors.
First, we extract a dataset of 6 common descriptors: Drainage area (AREA), mean annual precipitation (MAP), percentage of water bodies (WB), stream density (STREAM), elevation (ELEV) and slope (SLOPE).
We apply the logarithmic transformations to some input variables to reshape them in an approximately normal distribution. 

```{r}
gauged <- with(descriptors, 
    data.frame(
      site = station,
    area = log(area),
    map  = log(map_ws),
    wb   = log(.01 + wb),
    stream = log(.01 + stream),
      elev = elev_ws,
      slope = log(.01 + slope)))

## Separating the target from the other stations
target.id <- which(gauged$site == '01AF009')

target <- gauged[target.id,]
gauged <- gauged[-target.id,]
```

The prediction of flood quantiles for one or more ungauged sites is performed using the function `FloodnetRoi`, which is built on the top of the function `FitRoi` in `CSHShydRology`.
In addition to the catchment descriptors, the function requires the annual maxima of all gauged sites. 
In the example below, this is provided by the function `AmaxData`.

```{r}
set.seed(1)
fit <- DB_HYDAT %>%
    AmaxData(gauged$site) %>%
    FloodnetRoi(target = target, sites = gauged, period = 100, size = 30)
```
 
Briefly, the procedure starts by evaluating the desired flood quantiles $\mathbf{q} = (q_1, \ldots, q_n$ for each gauged station using AMAX and the results are passed to a locally log-linear model
$$
\log(\mathbf{q}) = \mathbf{X}\beta + \mathbf{e}
$$
where $\mathbf{X}$ is a design matrix of descriptors, $\beta$ is a vector of parameters and $\mathbf{e}$ is a term of errors.
For each target site, a regression model is fitted using a weighted least-squares approach that gives more weights to the nearest stations. 
The Euclidean distance between standardized descriptors serves to evaluate these weights according to the Epanechnikov kernel. 
The regression model requires the calibration of a bandwidth parameter that controls the weight decay and corresponds to the radius of a region of influence (ROI) outside which the weights become zero.
This parameter is expressed in `FloodnetRoi` as the rank of the nearest gauged sites to the target, which can be interpreted as the size of pooling group that contributes to the prediction of the target site.

Similarly to the outputs of the gauged analyses, interpreting functions exist to extract information from the output of the ungauged analyses. 
The estimated flood quantiles can be display using the function `print` or by converting the model to a dataset with the function `as.data.frame`.
Remember that we estimated Q100 equal to 98 $m^3/s$ using the AMAX method and we predict here 89 $m^3/s$ based on its catchment descriptors.

```{r}
print(fit)
as.data.frame(fit)
```

After fitting the model, a 10-fold cross-validation resampling strategy is used to assess the quality of the fitting.
The strategy consists to divide the set of gauged sites into 10 groups of approximately equal sizes. 
In turn, each validation group is treated as ungauged and their flood quantiles are estimated.
Note that the validation groups are divided at random and thus, the outcome of the cross-validation procedure will differ at each function call.

If more than one pooling group size is provided, the function determines the selected size by cross-validation.
The Nash-Sutcliffe criterion or NASH is a prediction skill score that rescales the mean square error into a unitless measure that quantifies the model performance with respect to the sample average.
A NASH of 1 indicates a perfect score, while a score close to zero indicates a poor performance.
To account for the scale of the watersheds, the NASH is applied here to the logarithm values.  
The same rescalling strategy is applied to derive the following prediction skill score 
$$
SKILL = 1- \frac{\sum_{i=1}^n |l_i-\hat l_i|}{\sum_{i=1}^n |l_i-\bar l_i|}
$$
where $l_i = \log(q_i)$, $\hat l_i$ is a predicted value and $\bar l_i$ is the sample average. 
This criterion has the same forms as the NASH, except that the square differences are replaced by absolute deviations.
In the end, the selected pooling group size is the one that maximize the SKILL score among the list of candidates.

The results of the cross-validation procedure can be displayed using the function `summary`.
Note the small discrepancies between the two `nash` criteria in the output below due to different cross-validation samples used for the calibration and the evaluation of the QRT model.  

```{r}
fit <- DB_HYDAT %>%
	AmaxData(gauged$site) %>%
	FloodnetRoi(target = target, sites = gauged, period = 100, 
							size = seq(20, 120, 10), verbose = FALSE)

as.data.frame(fit)
summary(fit)
```

The two following graphics can be used to assess the estimated flood quantiles and the calibration of the pooling group size.

```{r}
plot(fit)
plot(fit, 'cv')
```

The first plot shows the at-site estimates of Q100 versus the values predicted for each cross-validation samples.
Deviation from the unitary line suggest systematic errors in the QRT model.
The second plot shows the evolution of the two cross-validation scores with respect to the pooling group size. 

To estimate uncertainty of the QRT model, the function `FloodnetRoi` uses a bootstrap resampling technique.
As for RFA, annual maxima are simulated using a multivariate normal distribution that results in a flood quantiles $\mathbf{q}_i^\ast$ at each gauged site $i$. 
In parallel, prediction errors $\mathbf{e}^\ast_i$ are sampled (balance bootstrap) from the predicted residuals of the QRT model during cross-validation.
The final bootstrap sample is composed of the flood quantiles values 
$\mathbf{q}_i^{\ast\ast} = \mathbf{q}_i^\ast+\mathbf{e}^\ast_i$ that account for both the modeling error and the sampling error.
The function `IntersiteCorData` can be used to evaluate intersite correlation matrix of annual maximum discharge according to an exponential correlation model.

```{r}
## Estimation of the intersite correlation matrix
icor <- IntersiteCorData(DB_HYDAT, gauged$site)

## Ungauged analysis with bootstrap
set.seed(1)
fit <- DB_HYDAT %>%
	AmaxData(gauged$site) %>%
	FloodnetRoi(target = target, sites = gauged, period = 100, size = 90, 
							nsim = 30, corr = icor)

print(fit)
```

Available catchment descriptors represent only partial information about the target catchment. 
In particular, several missing descriptors could be spatially distributed, which may lead to spatially correlated residuals.
If coordinates are provided to `FloodnetRoi`, the function performs a simple kriging of the residuals to improve the estimation of the flood quantiles. 

The code below use multidimensional scaling to project the geographical coordinates into a Cartesian space that approximately preserve the great-circle distance and feed the coordinates to the modeling functions.
The example below shows that the criterion SKILL improves from 0.72 to 0.75. 

```{r, }
library(CSHShydRology)

## Extract the coordinates
coord <- descriptors[, c('lon', 'lat')]
coord <- as.data.frame(cmdscale(GeoDist(coord), 2))

target.coord <- coord[target.id,]
coord <- coord[-target.id,]
 
## Predict target using ROI + simple kriging
set.seed(1)
fit <- DB_HYDAT %>%
	AmaxData(gauged$site) %>%
	FloodnetRoi(sites = gauged, target = target, sites.coord = coord, target.coord = target.coord, 
              size = 300, period = 100, verbose = FALSE)

summary(fit)
```

## Conclusion

In summary, this document showed how the R-package `floodnetRfa` can be used to perform FFA using the hydrometric data found in the HYDAT database.
Data extraction functions `AmaxData`, `DailyData` and `DailyPeakData` were shown to extract from HYDAT the hydrometric data in the correct format.
The outputs were passed to the modeling functions `FloodnetAmax`, `FloodnetPot` and `FloodnetPool`that carried out FFA based on AMAX, POT, and pooling groups.
Finally, interpreting the functions `print`, `summary`, `as.data.frame` and `plot` were used to extract the estimated flood quantiles and assess the fitted model.
Finally, flood quantiles were estimated at ungauged sites using the `FloodnetRoi`.

To facilitate some steps of the different FFA methods, two datasets were introduced: `gaugedSites`, `descriptors`.
The provided information included candidate thresholds for POT analysis, super regions and trend tests for creating pooling groups and catchment descriptors for estimating flood quantile at ungauged sites. 
For more code and examples related to the R-package `floodnetRfa`, please see the supplementary material in [floodnetRfa_extra](https://github.com/floodnetProject16/floodnetRfa_extra).


## References

* Coles, S. (2001). An introduction to statistical modeling of extreme values. Springer Verlag.

* Durocher, M., Burn, D. H., & Mostofi Zadeh, S. (2018a). A nationwide regional flood frequency analysis at ungauged sites using ROI/GLS with copulas and super regions. Journal of Hydrology, 567, 191–202. https://doi.org/10.1016/j.jhydrol.2018.10.011

* Durocher, M., Zadeh, S. M., Burn, D. H., & Ashkar, F. (2018b). Comparison of automatic procedures for selecting flood peaks over threshold based on goodness-of-fit tests. Hydrological Processes. https://doi.org/10.1002/hyp.13223

* Durocher, M., Burn, D. H., Zadeh, S. M., & Ashkar, F. (2019). Estimating flood quantiles at ungauged sites using nonparametric regression methods with spatial components. Hydrological Sciences Journal, 64(9), 1056–1070. https://doi.org/10.1080/02626667.2019.1620952

* Helsel, D. R., & Hirsch, R. M. (2002). Statistical Methods in Water Resources. In Techniques of Water-Resources Investigations of the United States Geological Survey. Retrieved from http://water.usgs.gov/pubs/twri/twri4a3/

* Hosking, J. R. M., & Wallis, J. R. (1997). Regional frequency analysis: An approach based on L-moments. Cambridge Univ Pr.

* Lang, M., Ouarda, T. B. M. J., & Bobée, B. (1999). Towards operational guidelines for over-threshold modeling. Journal of Hydrology, 225(3), 103–117. https://doi.org/10.1016/S0022-1694(99)00167-5

* Mostofi Zadeh, S., & Burn, D. H. (2019). A Super Region Approach to Improve Pooled Flood Frequency Analysis. Canadian Water Resources Journal / Revue Canadienne Des Ressources Hydriques, 0(0), 1–14. https://doi.org/10.1080/07011784.2018.1548946

* Robson, A., & Reed, D. (1999). Flood estimation handbook. Institute of Hydrology, Wallingford.


